---
title: "Low-Resource Neural Machine Translation Using Recurrent Neural Networks
  and Transfer Learning: A Case Study on English-to-Igbo"
date: April 24, 2025 at 05:02 AM UTC
authors: "Ocheme Anthony Ekle, Biswarup Das"
arxiv_id: "2504.17252v1"
pdf: "http://arxiv.org/pdf/2504.17252v1"
topic: "machine translation"
---

In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.
